

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-108114467-1', 'auto');
      ga('send', 'pageview');
  </script>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link href='https://fonts.googleapis.com/css?family=Titillium Web' rel='stylesheet'>
  <style type="text/css">
    /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 400
    }
    heading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 19px;
      font-weight: 1000
    }
    strong {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 1200
    }
    strongred {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      color: 'red' ;
      font-size: 16px
    }
    sectionheading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 22px;
      font-weight: 600
    }
    </style>
  <link rel="shortcut icon" type="image/png" href="images/profile.png" />
  <title>Sagnik Majumder</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="20">
    <tr>
        <td halign="center">
          <p align="center">
            <font size="6">Sagnik Majumder</font>

          </p>
        </td>
      </tr>
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p>I am a PhD student in Computer Science at UT Austin, advised by <a href="https://www.cs.utexas.edu/users/grauman/">Prof.&nbsp;Kristen&nbsp;Grauman</a>. Before this, I received my MS in Computer Science at UT. I am broadly interested in computer vision and machine learning. My current line of research is embodied audio-visual understanding of 3D scenes with applications in mobile robotics and AR/VR.

        </p>
        <p>


           Previously, I have worked with <a href="http://www.ccc.cs.uni-frankfurt.de/people/">Prof.&nbsp;Visvanathan&nbsp;Ramesh</a> at <a href="https://www.goethe-university-frankfurt.de/en?legacy_request=1">Goethe&nbsp;University</a> on continual and meta learning for image recognition tasks. I have also had the pleasure of collaborating with <a href="https://www.fias.science/en/neuroscience/research-groups/christoph-von-der-malsburg/">Prof.&nbsp;Christoph&nbsp;Malsburg</a> at the <a href="https://fias.institute/en/">Frankfurt&nbsp;Institute&nbsp;for&nbsp;Advanced&nbsp;Studies</a> for investigating visual models that draw motivation from Neuroscience. 

        </p>
        <p>Earlier, I graduated from <a href="https://www.bits-pilani.ac.in/">BITS&nbsp;Pilani</a>.

<p><strong>Research collaborations:</strong> I am open to collaborating on research projects, and also mentoring Master's and final (senior) year students on their thesis projects. Shoot me an email to discuss more.</p>

            <p align="center"><a>

        <!--<p><strong>Research collaborations:</strong> I am open to collaborating on research projects, and also mentoring Master's and final (senior) year undergraduate students on their thesis projects. Shoot me an email to discuss more.</p>
-->      </a><a href="pdfs/CV_long_3pg.pdf">CV</a> | </a><a href=mailto:sagnik@cs.utexas.edu>E-Mail</a> | <a href="https://scholar.google.co.in/citations?user=cY4Q2FAAAAAJ&hl=en">Google Scholar</a> | <a href="https://github.com/SAGNIKMJR">Github</a> | <a href="https://twitter.com/sagnikmjr"> Twitter</a>
      </p>

        </td>
        <td width="100%" valign="top">
        <img src="images/profile.png" width="120%">
        </td>
      </tr>
      </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <heading>Affiliations</heading>
        </td>
      </tr>
      </tbody></table>
    <table align="center">
        <tbody>
        <tr>
            <td width="22%" align="center">
                <a href="https://www.bits-pilani.ac.in/" target="_blank">
                <img style="width:120px"  src="images/bits.png"></a>&nbsp &nbsp
            </td>
            <td width="22%" align="center">
                <a href="https://fias.institute/en/" target="_blank">
                <img style="width:120px"  src="images/fias.png"></a>&nbsp &nbsp
            </td>
            <td width="22%" align="center">
                <a href="https://www.goethe-university-frankfurt.de/en?legacy_request=1" target="_blank">
                <img style="width:120px" src="images/goethe.png"></a>&nbsp &nbsp
            </td>
            <td width="22%" align="center">
                <a href="https://ai.meta.com/" target="_blank">
                <img style="width:120px" src="images/metaAI_v2.png"></a>&nbsp &nbsp
            </td>
            <td width="22%" align="center">
                <a href="https://www.utexas.edu/" target="_blank">
                <img style="width:120px" src="images/ut.png"></a>&nbsp &nbsp
            </td>
        </tr>
        <tr>
            <td width="22%" align="center"><font size="3">BITS Pilani<br>2014-2018</font></td>
            <td width="22%" align="center"><font size="3">FIAS<br>Summer 2017</font></td>
            <td width="22%" align="center"><font size="3">Goethe University<br>2018-2019</font></td>
	    <td width="22%" align="center"><font size="3">Meta AI<br>2022-present</font></td>
            <td width="22%" align="center"><font size="3">UT Austin<br>2019-present</font></td>
        </tr>
        </tbody>
    </table>
    <br>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>News</heading>
        </td>
      </tr>
      </table>	 
                <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
                    <colgroup>
                        <col width="15%">
                        <col width="85%">
                    </colgroup>
                    <tbody> 

  <tr>

        <td valign="top" align="center"><strong>Mar 2023</strong></td>
        <td>Our paper <a href="https://vision.cs.utexas.edu/projects/chat2map/">Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations</a> got accepted at CVPR 23!</td>
    </tr>

  <tr>

        <td valign="top" align="center"><strong>Feb 2023</strong></td>
        <td>Co-organzing the <a href="https://soundspaces.org/challenge">SoundSpaces Challenge</a> at the <a href="http://embodied-ai.org/">CVPR 2023 Embodied AI Workshop</a>.</td>
    </tr>

  <tr>
        <td valign="top" align="center"><strong>June 2023</strong></td>
        <td>Invited talk at <a href="https://sightsound.org/">CVPR 23 Sight and Sound Workshop</a>, "Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations".</td>
    </tr>


  <tr>
        <td valign="top" align="center"><strong>May 2023</strong></td>
        <td>Invited talk at <a href="https://engineering.jhu.edu/nsa/">JHU NSA Lab</a>, "Efficiently understanding 3D scenes using sight and sound".</td>
    </tr>


  <tr>

        <td valign="top" align="center"><strong>Mar 2023</strong></td>
        <td>Our paper <a href="https://vision.cs.utexas.edu/projects/chat2map/">Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations.</a> got accepted at CVPR 23!</td>
    </tr>


  <tr>

        <td valign="top" align="center"><strong>Feb 2023</strong></td>
        <td>Co-organzing the CVPR 2023 <a href="https://soundspaces.org/challenge">SoundSpaces Challenge</a> and the <a href="http://embodied-ai.org/">Embodied AI Workshop.</td>
    </tr>




  <tr>

        <td valign="top" align="center"><strong>Dec 2022</strong></td>
        <td>Starting as a visiting researcher at <a href="https://ai.facebook.com//">Meta AI Research</a>.</td>
    </tr>

  <tr>
        <td valign="top" align="center"><strong>Oct 2022</strong></td>
        <td>Invited talk at <a href="https://av4d.org/eccv22">ECCV 22 AV4D Workshop</a>, "Active Audio-Visual Separation of Dynamic Sound Sources".</td>
    </tr>


  <tr>

        <td valign="top" align="center"><strong>Sept 2022</strong></td>
        <td>Our paper <a href="https://vision.cs.utexas.edu/projects/fs_rir/">Few-Shot Audio-Visual Learning of Environment Acoustics</a> got accepted at NeurIPS 22!</td>
    </tr>


  <tr>
        <td valign="top" align="center"><strong>Sept 2022</strong></td>
        <td>Continuing as a student researcher at <a href="https://about.facebook.com/realitylabs/">Meta Reality Labs Redmond</a> this Fall.</td>
    </tr>


  <tr>

        <td valign="top" align="center"><strong>July 2022</strong></td>
        <td>Our paper <a href="https://vision.cs.utexas.edu/projects/active-av-dynamic-separation/">Active Audio-Visual Separation of Dynamic Sound Sources</a> got accepted at ECCV 22!</td>
    </tr>

   

  <tr>
        <td valign="top" align="center"><strong>June 2022</strong></td>
        <td>Invited talk at <a hred="https://sightsound.org/2022">CVPR 22 Sight and Sound Workshop</a>, "Active Audio-Visual Separation of Dynamic Sound Sources" (<a href="https://drive.google.com/file/d/12DzafzyrS7Od0aVgLDAPoXzi4lbwBDei/view?usp=sharing">Slides</a>).</td>
    </tr>


  <tr>
        <td valign="top" align="center"><strong>June 2022</strong></td>
        <td>Joined <a href="https://about.facebook.com/realitylabs/">Meta Reality Labs Redmond</a> as a research scientist intern this summer.</td>
    </tr>

  <tr>
        <td valign="top" align="center"><strong>April 2022</strong></td>
        <td>Invited talk at <a hred="https://ai.facebook.com/">Meta AI Research</a>, "Active Audio-Visual Separation of Dynamic Sound Sources" (<a href="https://drive.google.com/file/d/12DzafzyrS7Od0aVgLDAPoXzi4lbwBDei/view?usp=sharing">Slides</a>).</td>
    </tr>

  <tr>

        <td valign="top" align="center"><strong>Feb 2022</strong></td>
        <td>Co-organzing the <a href="https://soundspaces.org/challenge">SoundSpaces Challenge</a> at the <a href="http://embodied-ai.org/">CVPR 2022 Embodied AI Workshop</a>.</td>
    </tr>

  <tr>
        <td valign="top" align="center" ><strong>Jan 2022</strong></td>
        <td>Ported my old webpage to this new one. Will try to regulary update this space from now on!</td>
    </tr>


    <tr>
    </tbody></table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" STYLE="margin-top: 50px;">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publications</heading>
        </td>
      </tr>
      </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr onmouseout="retro_egoAVCorr_start()" onmouseover="retro_egoAVCorr_stop()">
            <td width="35%">
                <div class="one">
                    <div class="two" id='retro_egoAVCorr'><img src='images/egoAVCorr.png' alt="sym" width="100%"
                                                    style="border-style: none"></div>
                </div>
                <script type="text/javascript">
                    function retro_egoAVCorr_start() {
                        document.getElementById('retro_egoAVCorr').style.opacity = "0.9";
                    }

                    function retro_egoAVCorr_stop() {
                        document.getElementById('retro_egoAVCorr').style.opacity = "1";
                    }

                    friendly_stop()
                </script>
            </td>
            <td valign="top" width="65%">
                <p><a href="">
                    <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                    <heading>Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos</heading>
                </a><br>
                    <strong>Sagnik Majumder</strong>, Ziad Al-Halah, Kristen Grauman<br>
                    <!--<em>*Equal contribution<br></em>-->
                    <em>arXiv<br></em>
                    <a href="https://arxiv.org/pdf/2307.04760.pdf">paper</a> |
                    <a href="https://vision.cs.utexas.edu/projects/ego_av_corr/">project</a>
                    <!--<a href="https://github.com/SAGNIKMJR/few-shot-rir">code and data</a>-->
            </td>
        </tr>



	<tr onmouseout="chat2map_start()" onmouseover="chat2map_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='chat2map'><img src='images/chat2map.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function chat2map_start() {
		        document.getElementById('chat2map').style.opacity = "0.9";
		    }

		    function chat2map_stop() {
		        document.getElementById('chat2map').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
		    <heading>Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations</heading>
		</a><br>
		    <strong>Sagnik Majumder</strong>, Hao Jiang, Pierre Moulon, Ethan Henderson, Paul Calamia, Kristen Grauman*, Vamsi Krishna Ithapu*<br>
		    <em>*Equal contribution<br></em>
		    <em>CVPR 2023<br></em>
		    <a href="https://arxiv.org/pdf/2301.02184.pdf">paper</a> |

		    <a href="https://vision.cs.utexas.edu/projects/chat2map/">project</a> |
                    <a href="https://github.com/SAGNIKMJR/chat2map">code and data (coming soon!)</a>

		    <!--<a href="https://vision.cs.utexas.edu/projects/chat2map/">project</a>-->
        <!--            <a href="https://github.com/SAGNIKMJR/few-shot-rir">code and data</a>-->
	    </td>
	</tr>


	<tr onmouseout="retro_embodiedAI_start()" onmouseover="retro_embodiedAI_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='retro_embodiedAI'><img src='images/retro_embodiedAI.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function retro_embodiedAI_start() {
		        document.getElementById('retro_embodiedAI').style.opacity = "0.9";
		    }

		    function retro_embodiedAI_stop() {
		        document.getElementById('retro_embodiedAI').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Retrospectives on the Embodied AI Workshop</heading>
		</a><br>
		    Matt Deitke, Dhruv Batra, Yonatan Bisk, ..., <strong>Sagnik Majumder</strong>, ..., Luca Weihs, Jiajun Wu<br>
		    <!--<em>*Equal contribution<br></em>-->
		    <em>arXiv<br></em>
		    <a href="https://arxiv.org/pdf/2210.06849.pdf">paper</a>
		    <!--<a href="https://vision.cs.utexas.edu/projects/fs_rir/">project</a> |
                    <a href="https://github.com/SAGNIKMJR/few-shot-rir">code and data</a>-->
	    </td>
	</tr>

	<tr onmouseout="fs_rir_start()" onmouseover="fs_rir_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='fs_rir'><img src='images/fs_rir.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function fs_rir_start() {
		        document.getElementById('fs_rir').style.opacity = "0.9";
		    }

		    function davis_stop() {
		        document.getElementById('fs_rir').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Few-Shot Audio-Visual Learning of Environment Acoustics</heading>
		</a><br>
		    <strong>Sagnik Majumder</strong>, Changan Chen*, Ziad Al-Halah*, Kristen Grauman<br>
		    <em>*Equal contribution<br></em>
		    <em>NeurIPS 2022<br></em>
		    <a href="https://arxiv.org/pdf/2206.04006.pdf">paper</a> |
		    <a href="https://vision.cs.utexas.edu/projects/fs_rir/">project</a> |
                    <a href="https://github.com/SAGNIKMJR/few-shot-rir">code and data</a>
	    </td>
	</tr>

	<tr onmouseout="davis_start()" onmouseover="davis_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='davis'><img src='images/davis.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function davis_start() {
		        document.getElementById('davis').style.opacity = "0.9";
		    }

		    function davis_stop() {
		        document.getElementById('davis').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Active Audio-Visual Separation of Dynamic Sound Sources</heading>
		</a><br>
		    <strong>Sagnik Majumder</strong>, Kristen Grauman<br>
		    <em>ECCV 2022<br></em>
		    <a href="https://arxiv.org/pdf/2202.00850.pdf">paper</a> |
		    <a href="https://vision.cs.utexas.edu/projects/active-av-dynamic-separation/">project</a> |
		    <a href="https://vision.cs.utexas.edu/projects/active-av-dynamic-separation/">code and data</a>
	    </td>
	</tr>


	<tr onmouseout="move2hear_start()" onmouseover="move2hear_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='move2hear'><img src='images/move2hear.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function move2hear_start() {
		        document.getElementById('move2hear').style.opacity = "0.9";
		    }

		    function move2hear_stop() {
		        document.getElementById('move2hear').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Move2Hear: Active Audio-Visual Source Separation</heading>
		</a><br>
		    <strong>Sagnik Majumder</strong>, Ziad Al-Halah, Kristen Grauman<br>
		    <em>ICCV 2021<br></em>
		    <a href="https://arxiv.org/pdf/2105.07142.pdf">paper</a> |
		    <a href="https://vision.cs.utexas.edu/projects/move2hear/">project</a> |
		    <a href="https://github.com/SAGNIKMJR/move2hear-active-AV-separation">code and data</a>
	    </td>
	</tr>


	<tr onmouseout="avwan_start()" onmouseover="avwan_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='avwan'><img src='images/avwan.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function avwan_start() {
		        document.getElementById('avwan').style.opacity = "0.9";
		    }

		    function avwan_stop() {
		        document.getElementById('avwan').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Learning to Set Waypoints for Audio-Visual Navigation</heading>
		</a><br>
		    Changan Chen, <strong>Sagnik Majumder</strong>, Ziad Al-Halah, Ruohan Gao, Santhosh K. Ramakrishnan, Kristen Grauman<br>
		    <em>ICLR 2021<br></em>
		    <a href="https://arxiv.org/pdf/2008.09622.pdf">paper</a> |
		    <a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">project</a> |
		    <a href="https://github.com/facebookresearch/sound-spaces/tree/master/ss_baselines/av_wan">code</a>
	    </td>
	</tr>


	<tr onmouseout="maars_start()" onmouseover="maars_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='maars'><img src='images/maars.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function maars_start() {
		        document.getElementById('maars').style.opacity = "0.9";
		    }

		    function maars_stop() {
		        document.getElementById('maars').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Model Agnostic Answer Reranking System for Adversarial Question Answering</heading>
		</a><br>
		    <strong>Sagnik Majumder</strong>, Chinmoy Samant, Greg Durrett<br>
		    <em>EACL SRW 2021<br></em>
		    <a href="https://arxiv.org/pdf/2102.03016.pdf">paper</a> 
		    <!--<a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">project</a>
		    <a href="https://github.com/facebookresearch/sound-spaces/tree/master/ss_baselines/av_wan">code and data</a> -->
	    </td>
	</tr>


	<tr onmouseout="aerobi_start()" onmouseover="aerobi_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='aerobi'><img src='images/aerobi.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function aerobi_start() {
		        document.getElementById('aerobi').style.opacity = "0.9";
		    }

		    function aerobi_stop() {
		        document.getElementById('aerobi').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Meta-learning Convolutional Neural Architectures for Multi-target Concrete Defect Classification with the COncrete DEfect BRidge IMage Dataset</heading>
		</a><br>
		    Martin Mundt, <strong>Sagnik Majumder</strong>, Sreenivas Murali, Panagiotis Panetsos, Visvanathan Ramesh<br>
		    <em>CVPR 2021<br></em>
		    <a href="https://arxiv.org/pdf/1904.08486.pdf">paper</a> |
		    <!--<a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">project</a>-->
		    <a href="https://github.com/MrtnMndt/meta-learning-CODEBRIM">code and data</a>
	    </td>
	</tr>

	<tr onmouseout="ocdvae_start()" onmouseover="ocdvae_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='ocdvae'><img src='images/ocdvae.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function ocdvae_start() {
		        document.getElementById('ocdvae').style.opacity = "0.9";
		    }

		    function ocdvae_stop() {
		        document.getElementById('ocdvae').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition</heading>
		</a><br>
		    Martin Mundt, <strong>Sagnik Majumder</strong>, Iuliia Pliushch, Visvanathan Ramesh<br>
		    <em>arXiv 2019<br></em>
		    <a href="https://arxiv.org/pdf/1905.12019.pdf">paper</a> |
		    <!--<a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">project</a>-->
		    <a href="https://github.com/MrtnMndt/OCDVAEContinualLearning">code</a>
	    </td>
	</tr>

	<tr onmouseout="ocdvaeW_start()" onmouseover="ocdvaeW_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='ocdvaeW'><img src='images/ocdvaeW.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function ocdvaeW_start() {
		        document.getElementById('ocdvaeW').style.opacity = "0.9";
		    }

		    function ocdvaeW_stop() {
		        document.getElementById('ocdvaeW').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Open Set Recognition Through Deep Neural Network Uncertainty: Does Out-of-Distribution Detection Require Generative Classifiers?</heading>
		</a><br>
		    Martin Mundt, Iuliia Pliushch, <strong>Sagnik Majumder</strong>, Visvanathan Ramesh<br>
		    <em>ICCV SDLCV Workshop 2019<br></em>
		    <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/SDL-CV/Mundt_Open_Set_Recognition_Through_Deep_Neural_Network_Uncertainty_Does_Out-of-Distribution_ICCVW_2019_paper.pdf">paper</a> |
		    <!--<a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">project</a>-->
		    <a href="https://github.com/MrtnMndt/Deep_Openset_Recognition_through_Uncertainty">code</a>
	    </td>
	</tr>


	<tr onmouseout="cract_start()" onmouseover="cract_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='cract'><img src='images/cract.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function cract_start() {
		        document.getElementById('cract').style.opacity = "0.9";
		    }

		    function cract_stop() {
		        document.getElementById('cract').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Rethinking Layer-wise Feature Amounts in Convolutional Neural Network Architectures</heading>
		</a><br>
		    Martin Mundt, <strong>Sagnik Majumder</strong>, Tobias Weis, Visvanathan Ramesh<br>
		    <em>NeurIPS CRACT Workshop 2018<br></em>
		    <a href="https://arxiv.org/pdf/1812.05836.pdf">paper</a> |
		    <!--<a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">project</a>-->
		    <a href="https://github.com/MrtnMndt/Rethinking_CNN_Layerwise_Feature_Amounts">code</a>
	    </td>
	</tr>



	<tr onmouseout="handwritten_start()" onmouseover="handwritten_stop()">
	    <td width="35%">
		<div class="one">
		    <div class="two" id='cract'><img src='images/handwritten.png' alt="sym" width="100%"
		                                    style="border-style: none"></div>
		</div>
		<script type="text/javascript">
		    function handwritten_start() {
		        document.getElementById('handwritten').style.opacity = "0.9";
		    }

		    function handwritten_stop() {
		        document.getElementById('handwritten').style.opacity = "1";
		    }

		    friendly_stop()
		</script>
	    </td>
	    <td valign="top" width="65%">
		<p><a href="">
		    <!--<img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
		    <heading>Handwritten Digit Recognition by Elastic Matching</heading>
		</a><br>
		    <strong>Sagnik Majumder</strong>, Christoph von der Malsburg, Aashish Richhariya, Surekha Bhanot<br>
		    <em>JCP 2018<br></em>
		    <a href="https://arxiv.org/pdf/1807.09324.pdf">paper</a> |
		    <!--<a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">project</a>-->
		    <a href="https://github.com/SAGNIKMJR/HandwrittenDigitRecognition_ElasticMatching_Python">code</a>
	    </td>
	</tr>

        
        
        
     
      </table>

  <!--<table width="100%" align="center" border="0" cellpadding="20">-->
      <!--<tr>-->
        <!--<td width="25%"><img src="ta.jpg" alt="numerical" width="160" height="160"></td>-->
        <!--<td width="75%" valign="center">-->
        <!--<p>-->
          <!--<a href="https://relate.cs.illinois.edu/course/cs357-f16/">-->
          <!--<papertitle>CS357 - Numerical Methods - Fall 2016 </papertitle>-->
          <!--</a>-->
          <!--<br><br>-->
          <!--<a href="https://go.illinois.edu/cs101">-->
          <!--<papertitle>CS101 - Intro to Computing - Spring 2016, Fall 2017 </papertitle>-->
          <!--</a>-->
          <!--<br>-->
        <!--</p>-->
        <!--</td>-->
      <!--</tr>-->
  <!--</table>-->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>News</heading>
        </td>
      </tr>
      </table>	 
                <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
                    <colgroup>
                        <col width="15%">
                        <col width="85%">
                    </colgroup>
                    <tbody> 


  <tr>

        <td valign="top" align="center"><strong>July 2022</strong></td>
        <td>Our paper <a href="https://vision.cs.utexas.edu/projects/active-av-dynamic-separation/">Active Audio-Visual Separation of Dynamic Sound Sources</a> got accepted at ECCV 22!</td>
    </tr>

   

  <tr>
        <td valign="top" align="center"><strong>June 2022</strong></td>
        <td>Invited talk at <a hred="https://sightsound.org/">CVPR 22 Sight and Sound Workshop</a>, "Active Audio-Visual Separation of Dynamic Sound Sources" (<a href="https://drive.google.com/file/d/12DzafzyrS7Od0aVgLDAPoXzi4lbwBDei/view?usp=sharing">Slides</a>).</td>
    </tr>


  <tr>
        <td valign="top" align="center"><strong>June 2022</strong></td>
        <td>Joined <a href="https://about.facebook.com/realitylabs/">Meta Reality Labs Redmond</a> as a research scientist intern this summer.</td>
    </tr>

  <tr>
        <td valign="top" align="center"><strong>April 2022</strong></td>
        <td>Invited talk at <a hred="https://ai.facebook.com/">Meta AI Research</a>, "Active Audio-Visual Separation of Dynamic Sound Sources" (<a href="https://drive.google.com/file/d/12DzafzyrS7Od0aVgLDAPoXzi4lbwBDei/view?usp=sharing">Slides</a>).</td>
    </tr>

  <tr>

        <td valign="top" align="center"><strong>Feb 2022</strong></td>
        <td>Co-organzing the <a href="https://soundspaces.org/challenge">SoundSpaces Challenge</a> at the <a href="http://embodied-ai.org/">CVPR 2022 Embodied AI Workshop</a>.</td>
    </tr>

  <tr>
        <td valign="top" align="center"><strong>Jan 2022</strong></td>
        <td>Ported my old webpage to this new one. Will try to regulary update this space from now on!</td>
    </tr>


    <tr>
    </tbody></table> -->



</div>
       <!--<table id="thanks" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--<tr>-->
              <!--<td>-->
                <!--<br>-->
                <!--<p align="right"><font size="2">-->
                  <!--<a href="http://www.cs.berkeley.edu/~barron/">(imitation is the sincerest form of flattery)</a>-->
                  <!--</font>-->
                <!--</p>-->
              <!--</td>-->
            <!--</tr>-->
          <!--</table>-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr><td><br><p align="right"><font size="2">
    Template credits: <a href="https://unnat.github.io/">Unnat</a>, <a href="https://changan.io/">Changan</a> and <a href="http://www.cs.berkeley.edu/~barron/">Jon</a>
    </font></p></td></tr>
</tbody></table>
     
    </td>
    </tr>
  </table>
  </body>
</html>

